import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import json
from typing import Set, List, Dict
import logging

class WebCrawler:
    def __init__(self, start_urls: List[str], allowed_domains: List[str] = None, 
                 max_pages: int = 100, delay: float = 1.0):
        """
        Initialize the web crawler
        
        Args:
            start_urls: List of URLs to start crawling from
            allowed_domains: List of allowed domains to crawl (optional)
            max_pages: Maximum number of pages to crawl
            delay: Delay between requests in seconds
        """
        self.start_urls = start_urls
        self.allowed_domains = set(allowed_domains) if allowed_domains else None
        self.max_pages = max_pages
        self.delay = delay
        self.visited_urls: Set[str] = set()
        self.results: List[Dict] = []
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def is_allowed_domain(self, url: str) -> bool:
        """Check if the URL's domain is in the allowed domains list"""
        if not self.allowed_domains:
            return True
        domain = urlparse(url).netloc
        return domain in self.allowed_domains

    def get_links(self, soup: BeautifulSoup, base_url: str) -> Set[str]:
        """Extract and normalize all links from the page"""
        links = set()
        for link in soup.find_all('a', href=True):
            url = urljoin(base_url, link['href'])
            if self.is_allowed_domain(url):
                links.add(url)
        return links

    def process_page(self, url: str, soup: BeautifulSoup) -> Dict:
        """
        Process the page content. Override this method to customize data extraction.
        """
        return {
            'url': url,
            'title': soup.title.string if soup.title else None,
            'text_content': soup.get_text()[:500],  # First 500 characters
            'links': len(soup.find_all('a')),
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }

    def crawl(self):
        """Main crawling method"""
        urls_to_visit = self.start_urls.copy()
        
        while urls_to_visit and len(self.visited_urls) < self.max_pages:
            url = urls_to_visit.pop(0)
            
            if url in self.visited_urls:
                continue
                
            self.logger.info(f"Crawling: {url}")
            
            try:
                # Add delay to be polite
                time.sleep(self.delay)
                
                # Make the request
                response = requests.get(url, timeout=10)
                response.raise_for_status()
                
                # Parse the page
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Process the page and store results
                result = self.process_page(url, soup)
                self.results.append(result)
                
                # Mark URL as visited
                self.visited_urls.add(url)
                
                # Add new URLs to the queue
                new_links = self.get_links(soup, url)
                urls_to_visit.extend([link for link in new_links 
                                    if link not in self.visited_urls])
                
            except Exception as e:
                self.logger.error(f"Error crawling {url}: {str(e)}")
                continue
                
        self.logger.info(f"Crawling completed. Visited {len(self.visited_urls)} pages.")

    def save_results(self, filename: str):
        """Save crawling results to a JSON file"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)

# Example usage
if __name__ == "__main__":
    # Initialize crawler
    crawler = WebCrawler(
        start_urls=['https://example.com'],
        allowed_domains=['example.com'],
        max_pages=50,
        delay=1.0
    )
    
    # Start crawling
    crawler.crawl()
    
    # Save results
    crawler.save_results('crawler_results.json')